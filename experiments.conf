###################################
#         Optimizer
###################################

adam {
  optimizer_type = AdamOptimizer
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100
  max_gradient_norm = 5.0
}
sgd {
  optimizer_type = GradientDescentOptimizer
  learning_rate = 0.1
  decay_rate = 0.999
  decay_frequency = 100
  max_gradient_norm = 5.0
}


###################################
#         Dataset
###################################
hsdataset{
  max_num_card = ${main.max_num_card}
  memory_size = 10000           # Number of replays required for an epoch.
  iterations_per_epoch = 2000  # Namely. At each step $batch_size examples are sampled from $memory_size examples.
}

###################################
#         Main
###################################
main {
  # Training parameter
  batch_size = 32
  optimizer = ${adam}
  max_epoch = 200
  max_to_keep = 1

  # NN stracture
  model_type = BagOfCards
  hidden_size = 100
  num_ff_layers = 2
  dropout_rate = 0.2 
  activation = relu

  # Dataset parameter
  dataset = ${hsdataset}

  # Q-learning parameter
  gamma = 0.99

  # Game parameter
  max_num_card = 3 # Picked cards more than or equal to this number are ignored when representing the current cards as state..
  vocab_size = {
    card = 160      # The number of card type.
  }
}
