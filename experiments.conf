###################################
#         Optimizer
###################################

adam {
  optimizer_type = AdamOptimizer
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100
  max_gradient_norm = 5.0
}

sgd {
  optimizer_type = GradientDescentOptimizer
  learning_rate = 0.1
  decay_rate = 0.999
  decay_frequency = 100
  max_gradient_norm = 5.0
}


###################################
#         Dataset
###################################
hsdataset{
  #dataset_type = HSReplayDataset
  #dataset_type = HSReplayDataset
  max_num_card = ${main.max_num_card}
  num_next_candidates_samples = ${main.num_next_candidates_samples}
  memory_size = 10000           # Number of replays required for an epoch.
  iterations_per_epoch = 1000  # Namely. At each step $batch_size examples are sampled from $memory_size examples.
  td_lambda = 0.977 # 0.977**30 ~= 0.5
  td_gamma = ${main.td_gamma}
}

###################################
#         Main
###################################
main {
  # Training parameter
  batch_size = 256
  optimizer = ${adam}
  max_epoch = 200
  max_to_keep = 1

  # Dataset parameter
  dataset = ${hsdataset}

  # NN stracture
  model_type = NStepTD
  #model_type = TDlambda
  hidden_size = 100
  num_ff_layers = 2
  dropout_rate = 0.2 
  hidden_activation = relu
  output_activation = tanh


  # Q-learning parameter
  td_gamma = 0.99 #0.99
  
  # Game parameter
  max_num_card = 3                 # Cards in deck more than this number are ignored when representing the current state.

  num_next_candidates_samples = 20 # The number of next candidates sampling. Here the next expected Q-values are approximated since it's too costly to consider all the patterns of the next 3 cards combinations.
  

  vocab_size = {
    card = 160      # The number of card type.
  }
}
