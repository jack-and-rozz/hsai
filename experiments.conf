###################################
#         Optimizer
###################################

adam {
  optimizer_type = AdamOptimizer
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100
  max_gradient_norm = 5.0
}
sgd {
  optimizer_type = GradientDescentOptimizer
  learning_rate = 0.1
  decay_rate = 0.999
  decay_frequency = 100
  max_gradient_norm = 5.0
}


###################################
#         Dataset
###################################
hsdataset{
  memory_size = 10000           # Number of replays required for an epoch.
  iterations_per_epoch = 30000  # Namely. At each step $batch_size examples are sampled from $memory_size examples.
}

###################################
#         Main
###################################
main {
  # Training parameter
  batch_size = 10
  optimizer = ${adam}
  max_epoch = 200
  max_to_keep = 1

  # NN stracture
  model_type = BagOfCards
  hidden_size = 100
  num_ff_layers = 2
  dropout_rate = 0.2 

  # Dataset parameter
  dataset = ${hsdataset}

  # Q-learning parameter
  gamma = 0.99

  # Game parameter
  vocab_size = {
    card = 160      # The number of card type.
  }
  max_num_card = 30  
}
