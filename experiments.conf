###################################
#         Optimizer
###################################

adam {
  optimizer_type = AdamOptimizer
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100
  max_gradient_norm = 5.0
}
sgd {
  optimizer_type = GradientDescentOptimizer
  learning_rate = 0.1
  decay_rate = 0.999
  decay_frequency = 100
  max_gradient_norm = 5.0
}



###################################
#         Main
###################################
main {
  # Training parameter
  batch_size = 32
  optimizer = ${adam}
  max_epoch = 200
  iterations_per_epoch = 10000

  # NN stracture
  model_type = BagOfCards
  hidden_size = 100
  num_ff_layers = 2
  dropout_rate = 0.2 

  # Dataset parameter
  memory_size = 50000

  # Q-learning parameter
  gamma = 0.99

  # Game parameter
  vocab_size = {
    card = 160      # The number of card type.
  }
  max_num_card = 30  
}
